{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークモデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDAで作成したデータファイル（``data_7.csv``）から特徴量を読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "      <th>IC_01</th>\n",
       "      <th>IC_07</th>\n",
       "      <th>IC_05</th>\n",
       "      <th>IC_16</th>\n",
       "      <th>...</th>\n",
       "      <th>CBN(13)_vs_DMN(94)</th>\n",
       "      <th>CBN(18)_vs_DMN(94)</th>\n",
       "      <th>CBN(4)_vs_DMN(94)</th>\n",
       "      <th>CBN(7)_vs_DMN(94)</th>\n",
       "      <th>CBN(18)_vs_CBN(13)</th>\n",
       "      <th>CBN(4)_vs_CBN(13)</th>\n",
       "      <th>CBN(7)_vs_CBN(13)</th>\n",
       "      <th>CBN(4)_vs_CBN(18)</th>\n",
       "      <th>CBN(7)_vs_CBN(18)</th>\n",
       "      <th>CBN(7)_vs_CBN(4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "      <td>5877.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15909.667007</td>\n",
       "      <td>50.034068</td>\n",
       "      <td>51.502462</td>\n",
       "      <td>59.304380</td>\n",
       "      <td>47.328355</td>\n",
       "      <td>51.910080</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.126329</td>\n",
       "      <td>0.340097</td>\n",
       "      <td>0.126801</td>\n",
       "      <td>0.299151</td>\n",
       "      <td>0.495320</td>\n",
       "      <td>0.578637</td>\n",
       "      <td>0.461299</td>\n",
       "      <td>0.197640</td>\n",
       "      <td>0.768528</td>\n",
       "      <td>0.354929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3411.775315</td>\n",
       "      <td>13.539881</td>\n",
       "      <td>9.801768</td>\n",
       "      <td>10.957016</td>\n",
       "      <td>11.087953</td>\n",
       "      <td>11.799972</td>\n",
       "      <td>0.004585</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254345</td>\n",
       "      <td>0.180884</td>\n",
       "      <td>0.238343</td>\n",
       "      <td>0.201886</td>\n",
       "      <td>0.197955</td>\n",
       "      <td>0.268321</td>\n",
       "      <td>0.254933</td>\n",
       "      <td>0.299296</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.182111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10001.000000</td>\n",
       "      <td>14.257265</td>\n",
       "      <td>15.769168</td>\n",
       "      <td>1.021874</td>\n",
       "      <td>0.991172</td>\n",
       "      <td>0.815285</td>\n",
       "      <td>-0.015894</td>\n",
       "      <td>-0.007958</td>\n",
       "      <td>-0.002240</td>\n",
       "      <td>-0.013459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.932657</td>\n",
       "      <td>-0.584421</td>\n",
       "      <td>-0.709769</td>\n",
       "      <td>-0.559527</td>\n",
       "      <td>-0.686442</td>\n",
       "      <td>-0.467751</td>\n",
       "      <td>-0.639171</td>\n",
       "      <td>-1.142909</td>\n",
       "      <td>-0.471138</td>\n",
       "      <td>-0.323693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12961.000000</td>\n",
       "      <td>40.129361</td>\n",
       "      <td>45.397852</td>\n",
       "      <td>53.133474</td>\n",
       "      <td>40.225097</td>\n",
       "      <td>44.586221</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>-0.001451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302299</td>\n",
       "      <td>0.230182</td>\n",
       "      <td>-0.026810</td>\n",
       "      <td>0.173736</td>\n",
       "      <td>0.374122</td>\n",
       "      <td>0.424559</td>\n",
       "      <td>0.307198</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.637085</td>\n",
       "      <td>0.243677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15925.000000</td>\n",
       "      <td>50.427747</td>\n",
       "      <td>51.847306</td>\n",
       "      <td>60.052535</td>\n",
       "      <td>47.811205</td>\n",
       "      <td>52.572032</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>0.009205</td>\n",
       "      <td>0.010567</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111208</td>\n",
       "      <td>0.357885</td>\n",
       "      <td>0.146985</td>\n",
       "      <td>0.316375</td>\n",
       "      <td>0.497220</td>\n",
       "      <td>0.579708</td>\n",
       "      <td>0.476942</td>\n",
       "      <td>0.247063</td>\n",
       "      <td>0.755655</td>\n",
       "      <td>0.362175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18886.000000</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>57.892677</td>\n",
       "      <td>66.521451</td>\n",
       "      <td>55.024768</td>\n",
       "      <td>59.843566</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.012035</td>\n",
       "      <td>0.012972</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061984</td>\n",
       "      <td>0.469599</td>\n",
       "      <td>0.300593</td>\n",
       "      <td>0.443144</td>\n",
       "      <td>0.615462</td>\n",
       "      <td>0.738983</td>\n",
       "      <td>0.624984</td>\n",
       "      <td>0.413300</td>\n",
       "      <td>0.881732</td>\n",
       "      <td>0.471903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21754.000000</td>\n",
       "      <td>84.491113</td>\n",
       "      <td>81.325580</td>\n",
       "      <td>94.702874</td>\n",
       "      <td>82.164478</td>\n",
       "      <td>94.509903</td>\n",
       "      <td>0.022888</td>\n",
       "      <td>0.027168</td>\n",
       "      <td>0.024085</td>\n",
       "      <td>0.022613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650448</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.780373</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.513935</td>\n",
       "      <td>2.123638</td>\n",
       "      <td>1.562309</td>\n",
       "      <td>1.102878</td>\n",
       "      <td>1.857374</td>\n",
       "      <td>1.282488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1463 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id          age  domain1_var1  domain1_var2  domain2_var1  \\\n",
       "count   5877.000000  5877.000000   5877.000000   5877.000000   5877.000000   \n",
       "mean   15909.667007    50.034068     51.502462     59.304380     47.328355   \n",
       "std     3411.775315    13.539881      9.801768     10.957016     11.087953   \n",
       "min    10001.000000    14.257265     15.769168      1.021874      0.991172   \n",
       "25%    12961.000000    40.129361     45.397852     53.133474     40.225097   \n",
       "50%    15925.000000    50.427747     51.847306     60.052535     47.811205   \n",
       "75%    18886.000000    59.580851     57.892677     66.521451     55.024768   \n",
       "max    21754.000000    84.491113     81.325580     94.702874     82.164478   \n",
       "\n",
       "       domain2_var2        IC_01        IC_07        IC_05        IC_16  ...  \\\n",
       "count   5877.000000  5877.000000  5877.000000  5877.000000  5877.000000  ...   \n",
       "mean      51.910080     0.005368     0.009237     0.010620     0.000895  ...   \n",
       "std       11.799972     0.004585     0.004162     0.003571     0.003587  ...   \n",
       "min        0.815285    -0.015894    -0.007958    -0.002240    -0.013459  ...   \n",
       "25%       44.586221     0.002445     0.006437     0.008172    -0.001451  ...   \n",
       "50%       52.572032     0.005512     0.009205     0.010567     0.000786  ...   \n",
       "75%       59.843566     0.008443     0.012035     0.012972     0.003207  ...   \n",
       "max       94.509903     0.022888     0.027168     0.024085     0.022613  ...   \n",
       "\n",
       "       CBN(13)_vs_DMN(94)  CBN(18)_vs_DMN(94)  CBN(4)_vs_DMN(94)  \\\n",
       "count         5877.000000         5877.000000        5877.000000   \n",
       "mean            -0.126329            0.340097           0.126801   \n",
       "std              0.254345            0.180884           0.238343   \n",
       "min             -0.932657           -0.584421          -0.709769   \n",
       "25%             -0.302299            0.230182          -0.026810   \n",
       "50%             -0.111208            0.357885           0.146985   \n",
       "75%              0.061984            0.469599           0.300593   \n",
       "max              0.650448            0.852686           0.780373   \n",
       "\n",
       "       CBN(7)_vs_DMN(94)  CBN(18)_vs_CBN(13)  CBN(4)_vs_CBN(13)  \\\n",
       "count        5877.000000         5877.000000        5877.000000   \n",
       "mean            0.299151            0.495320           0.578637   \n",
       "std             0.201886            0.197955           0.268321   \n",
       "min            -0.559527           -0.686442          -0.467751   \n",
       "25%             0.173736            0.374122           0.424559   \n",
       "50%             0.316375            0.497220           0.579708   \n",
       "75%             0.443144            0.615462           0.738983   \n",
       "max             0.912523            1.513935           2.123638   \n",
       "\n",
       "       CBN(7)_vs_CBN(13)  CBN(4)_vs_CBN(18)  CBN(7)_vs_CBN(18)  \\\n",
       "count        5877.000000        5877.000000        5877.000000   \n",
       "mean            0.461299           0.197640           0.768528   \n",
       "std             0.254933           0.299296           0.193878   \n",
       "min            -0.639171          -1.142909          -0.471138   \n",
       "25%             0.307198           0.022054           0.637085   \n",
       "50%             0.476942           0.247063           0.755655   \n",
       "75%             0.624984           0.413300           0.881732   \n",
       "max             1.562309           1.102878           1.857374   \n",
       "\n",
       "       CBN(7)_vs_CBN(4)  \n",
       "count       5877.000000  \n",
       "mean           0.354929  \n",
       "std            0.182111  \n",
       "min           -0.323693  \n",
       "25%            0.243677  \n",
       "50%            0.362175  \n",
       "75%            0.471903  \n",
       "max            1.282488  \n",
       "\n",
       "[8 rows x 1463 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_7 = pd.read_csv('data_7.csv')\n",
    "df_7.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークを使用するので、特徴量および予測値の両者を標準化する。  \n",
    "またfMRIデータ（相関係数）は数が多く、これに対する過学習を避けるため標準化したうえでさらに500で割り、特徴量の影響を軽減する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_7.iloc[:, 6:].values\n",
    "y = df_7.iloc[:, 1:6].values\n",
    "\n",
    "# train/validationに分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, random_state=1, train_size=0.75)\n",
    "\n",
    "# 標準化\n",
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "sc_x.fit(X_train)\n",
    "sc_y.fit(y_train)\n",
    "X_train_std = sc_x.transform(X_train)\n",
    "X_val_std = sc_x.transform(X_val)\n",
    "y_train_std = sc_y.transform(y_train)\n",
    "y_val_std = sc_y.transform(y_val)\n",
    "\n",
    "# fMRIデータ（86列目以降）は500で除す\n",
    "X_train_tsf = X_train_std.copy()\n",
    "X_train_tsf[:, 86:] = X_train_std[:, 86:]/500\n",
    "X_val_tsf = X_val_std.copy()\n",
    "X_val_tsf[:, 86:] = X_val_std[:, 86:]/500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerasを用いてモデル定義（ノード数：input:1463→720→360→180→output:5）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(720, activation = tf.nn.relu, input_shape=(1457,)),\n",
    "            tf.keras.layers.Dense(360, activation = tf.nn.relu),\n",
    "            tf.keras.layers.Dense(180, activation = tf.nn.relu),\n",
    "            tf.keras.layers.Dense(5, activation = tf.keras.activations.linear)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-foldを使用し、5通りのtrain/validationで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 720)               1049760   \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 360)               259560    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 180)               64980     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 905       \n",
      "=================================================================\n",
      "Total params: 1,375,205\n",
      "Trainable params: 1,375,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4701 samples, validate on 1176 samples\n",
      "Epoch 1/25\n",
      "4701/4701 [==============================] - 1s 265us/sample - loss: 1.4080 - mean_absolute_error: 1.4080 - val_loss: 0.7732 - val_mean_absolute_error: 0.7732\n",
      "Epoch 2/25\n",
      "4701/4701 [==============================] - 1s 132us/sample - loss: 0.7483 - mean_absolute_error: 0.7483 - val_loss: 0.9513 - val_mean_absolute_error: 0.9513\n",
      "Epoch 3/25\n",
      "4701/4701 [==============================] - 1s 133us/sample - loss: 0.7716 - mean_absolute_error: 0.7716 - val_loss: 0.7593 - val_mean_absolute_error: 0.7593\n",
      "Epoch 4/25\n",
      "4701/4701 [==============================] - 1s 135us/sample - loss: 0.7338 - mean_absolute_error: 0.7338 - val_loss: 0.7621 - val_mean_absolute_error: 0.7621\n",
      "Epoch 5/25\n",
      "4701/4701 [==============================] - 1s 137us/sample - loss: 0.7286 - mean_absolute_error: 0.7286 - val_loss: 0.7562 - val_mean_absolute_error: 0.7562\n",
      "Epoch 6/25\n",
      "4701/4701 [==============================] - 1s 137us/sample - loss: 0.7107 - mean_absolute_error: 0.7107 - val_loss: 0.7736 - val_mean_absolute_error: 0.7736\n",
      "Epoch 7/25\n",
      "4701/4701 [==============================] - 1s 131us/sample - loss: 0.7361 - mean_absolute_error: 0.7361 - val_loss: 0.7548 - val_mean_absolute_error: 0.7548\n",
      "Epoch 8/25\n",
      "4701/4701 [==============================] - 1s 137us/sample - loss: 0.7113 - mean_absolute_error: 0.7113 - val_loss: 0.7477 - val_mean_absolute_error: 0.7477\n",
      "Epoch 9/25\n",
      "4701/4701 [==============================] - 1s 136us/sample - loss: 0.7078 - mean_absolute_error: 0.7078 - val_loss: 0.7471 - val_mean_absolute_error: 0.7471\n",
      "Epoch 10/25\n",
      "4701/4701 [==============================] - 1s 136us/sample - loss: 0.6912 - mean_absolute_error: 0.6912 - val_loss: 0.7410 - val_mean_absolute_error: 0.7410\n",
      "Epoch 11/25\n",
      "4701/4701 [==============================] - 1s 136us/sample - loss: 0.6771 - mean_absolute_error: 0.6771 - val_loss: 0.7997 - val_mean_absolute_error: 0.7997\n",
      "Epoch 12/25\n",
      "4701/4701 [==============================] - 1s 129us/sample - loss: 0.7173 - mean_absolute_error: 0.7173 - val_loss: 0.7496 - val_mean_absolute_error: 0.7496\n",
      "Epoch 13/25\n",
      "4701/4701 [==============================] - 1s 125us/sample - loss: 0.6880 - mean_absolute_error: 0.6880 - val_loss: 0.7433 - val_mean_absolute_error: 0.7433\n",
      "Epoch 14/25\n",
      "4701/4701 [==============================] - 1s 128us/sample - loss: 0.6776 - mean_absolute_error: 0.6776 - val_loss: 0.7462 - val_mean_absolute_error: 0.7462\n",
      "Epoch 15/25\n",
      "4701/4701 [==============================] - 1s 134us/sample - loss: 0.6997 - mean_absolute_error: 0.6997 - val_loss: 0.7419 - val_mean_absolute_error: 0.7419\n",
      "Epoch 16/25\n",
      "4701/4701 [==============================] - 1s 133us/sample - loss: 0.6689 - mean_absolute_error: 0.6689 - val_loss: 0.7403 - val_mean_absolute_error: 0.7403\n",
      "Epoch 17/25\n",
      "4701/4701 [==============================] - 1s 155us/sample - loss: 0.6702 - mean_absolute_error: 0.6702 - val_loss: 0.7500 - val_mean_absolute_error: 0.7500\n",
      "Epoch 18/25\n",
      "4701/4701 [==============================] - 1s 174us/sample - loss: 0.6748 - mean_absolute_error: 0.6748 - val_loss: 0.7418 - val_mean_absolute_error: 0.7418\n",
      "Epoch 19/25\n",
      "4701/4701 [==============================] - 1s 181us/sample - loss: 0.6626 - mean_absolute_error: 0.6626 - val_loss: 0.7431 - val_mean_absolute_error: 0.7431\n",
      "Epoch 20/25\n",
      "4701/4701 [==============================] - 1s 205us/sample - loss: 0.6774 - mean_absolute_error: 0.6774 - val_loss: 0.7524 - val_mean_absolute_error: 0.7524\n",
      "Epoch 21/25\n",
      "4701/4701 [==============================] - 1s 198us/sample - loss: 0.6613 - mean_absolute_error: 0.6613 - val_loss: 0.7484 - val_mean_absolute_error: 0.7484\n",
      "Epoch 22/25\n",
      "4701/4701 [==============================] - 1s 177us/sample - loss: 0.6577 - mean_absolute_error: 0.6577 - val_loss: 0.7603 - val_mean_absolute_error: 0.7603\n",
      "Epoch 23/25\n",
      "4701/4701 [==============================] - 1s 156us/sample - loss: 0.6558 - mean_absolute_error: 0.6558 - val_loss: 0.7465 - val_mean_absolute_error: 0.7465\n",
      "Epoch 24/25\n",
      "4701/4701 [==============================] - 1s 155us/sample - loss: 0.6639 - mean_absolute_error: 0.6639 - val_loss: 0.7517 - val_mean_absolute_error: 0.7517\n",
      "Epoch 25/25\n",
      "4701/4701 [==============================] - 1s 150us/sample - loss: 0.6481 - mean_absolute_error: 0.6481 - val_loss: 0.7742 - val_mean_absolute_error: 0.7742\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 720)               1049760   \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 360)               259560    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 180)               64980     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 905       \n",
      "=================================================================\n",
      "Total params: 1,375,205\n",
      "Trainable params: 1,375,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4701 samples, validate on 1176 samples\n",
      "Epoch 1/25\n",
      "4701/4701 [==============================] - 1s 267us/sample - loss: 0.7051 - mean_absolute_error: 0.7051 - val_loss: 0.6914 - val_mean_absolute_error: 0.6914\n",
      "Epoch 2/25\n",
      "4701/4701 [==============================] - 1s 144us/sample - loss: 0.6880 - mean_absolute_error: 0.6880 - val_loss: 0.6837 - val_mean_absolute_error: 0.6837\n",
      "Epoch 3/25\n",
      "4701/4701 [==============================] - 1s 158us/sample - loss: 0.6919 - mean_absolute_error: 0.6919 - val_loss: 0.7066 - val_mean_absolute_error: 0.7066\n",
      "Epoch 4/25\n",
      "4701/4701 [==============================] - 1s 157us/sample - loss: 0.6808 - mean_absolute_error: 0.6808 - val_loss: 0.6853 - val_mean_absolute_error: 0.6853\n",
      "Epoch 5/25\n",
      "4701/4701 [==============================] - 1s 154us/sample - loss: 0.6839 - mean_absolute_error: 0.6839 - val_loss: 0.7045 - val_mean_absolute_error: 0.7045\n",
      "Epoch 6/25\n",
      "4701/4701 [==============================] - 1s 153us/sample - loss: 0.6816 - mean_absolute_error: 0.6816 - val_loss: 0.7181 - val_mean_absolute_error: 0.7181\n",
      "Epoch 7/25\n",
      "4701/4701 [==============================] - 1s 154us/sample - loss: 0.6888 - mean_absolute_error: 0.6888 - val_loss: 0.7247 - val_mean_absolute_error: 0.7247\n",
      "Epoch 8/25\n",
      "4701/4701 [==============================] - 1s 164us/sample - loss: 0.6978 - mean_absolute_error: 0.6978 - val_loss: 0.7060 - val_mean_absolute_error: 0.7060\n",
      "Epoch 9/25\n",
      "4701/4701 [==============================] - 1s 176us/sample - loss: 0.6943 - mean_absolute_error: 0.6943 - val_loss: 0.7199 - val_mean_absolute_error: 0.7199\n",
      "Epoch 10/25\n",
      "4701/4701 [==============================] - 1s 154us/sample - loss: 0.6890 - mean_absolute_error: 0.6890 - val_loss: 0.7698 - val_mean_absolute_error: 0.7698\n",
      "Epoch 11/25\n",
      "4701/4701 [==============================] - 1s 149us/sample - loss: 0.7535 - mean_absolute_error: 0.7535 - val_loss: 0.7533 - val_mean_absolute_error: 0.7533\n",
      "Epoch 12/25\n",
      "4701/4701 [==============================] - 1s 164us/sample - loss: 0.7270 - mean_absolute_error: 0.7270 - val_loss: 0.7287 - val_mean_absolute_error: 0.7287\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4701/4701 [==============================] - 1s 171us/sample - loss: 0.7093 - mean_absolute_error: 0.7093 - val_loss: 0.7368 - val_mean_absolute_error: 0.7368\n",
      "Epoch 14/25\n",
      "4701/4701 [==============================] - 1s 170us/sample - loss: 0.7100 - mean_absolute_error: 0.7100 - val_loss: 0.7309 - val_mean_absolute_error: 0.7309\n",
      "Epoch 15/25\n",
      "4701/4701 [==============================] - 1s 157us/sample - loss: 0.7100 - mean_absolute_error: 0.7100 - val_loss: 0.7217 - val_mean_absolute_error: 0.7217\n",
      "Epoch 16/25\n",
      "4701/4701 [==============================] - 1s 150us/sample - loss: 0.6880 - mean_absolute_error: 0.6880 - val_loss: 0.7227 - val_mean_absolute_error: 0.7227\n",
      "Epoch 17/25\n",
      "4701/4701 [==============================] - 1s 152us/sample - loss: 0.6807 - mean_absolute_error: 0.6807 - val_loss: 0.7257 - val_mean_absolute_error: 0.7257\n",
      "Epoch 18/25\n",
      "4701/4701 [==============================] - 1s 149us/sample - loss: 0.6923 - mean_absolute_error: 0.6923 - val_loss: 0.7248 - val_mean_absolute_error: 0.7248\n",
      "Epoch 19/25\n",
      "4701/4701 [==============================] - 1s 153us/sample - loss: 0.6925 - mean_absolute_error: 0.6925 - val_loss: 0.7218 - val_mean_absolute_error: 0.7218\n",
      "Epoch 20/25\n",
      "4701/4701 [==============================] - 1s 153us/sample - loss: 0.6891 - mean_absolute_error: 0.6891 - val_loss: 0.7217 - val_mean_absolute_error: 0.7217\n",
      "Epoch 21/25\n",
      "4701/4701 [==============================] - 1s 154us/sample - loss: 0.6922 - mean_absolute_error: 0.6922 - val_loss: 0.7193 - val_mean_absolute_error: 0.7193\n",
      "Epoch 22/25\n",
      "4701/4701 [==============================] - 1s 152us/sample - loss: 0.6867 - mean_absolute_error: 0.6867 - val_loss: 0.8684 - val_mean_absolute_error: 0.8684\n",
      "Epoch 00022: early stopping\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 720)               1049760   \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 360)               259560    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 180)               64980     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 905       \n",
      "=================================================================\n",
      "Total params: 1,375,205\n",
      "Trainable params: 1,375,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4702 samples, validate on 1175 samples\n",
      "Epoch 1/25\n",
      "4702/4702 [==============================] - 1s 261us/sample - loss: 0.7107 - mean_absolute_error: 0.7107 - val_loss: 0.7401 - val_mean_absolute_error: 0.7401\n",
      "Epoch 2/25\n",
      "4702/4702 [==============================] - 1s 141us/sample - loss: 0.7067 - mean_absolute_error: 0.7067 - val_loss: 0.6896 - val_mean_absolute_error: 0.6896\n",
      "Epoch 3/25\n",
      "4702/4702 [==============================] - 1s 153us/sample - loss: 0.6993 - mean_absolute_error: 0.6993 - val_loss: 0.6929 - val_mean_absolute_error: 0.6929\n",
      "Epoch 4/25\n",
      "4702/4702 [==============================] - 1s 151us/sample - loss: 0.6870 - mean_absolute_error: 0.6870 - val_loss: 0.6948 - val_mean_absolute_error: 0.6948\n",
      "Epoch 5/25\n",
      "4702/4702 [==============================] - 1s 151us/sample - loss: 0.6878 - mean_absolute_error: 0.6878 - val_loss: 0.6938 - val_mean_absolute_error: 0.6938\n",
      "Epoch 6/25\n",
      "4702/4702 [==============================] - 1s 155us/sample - loss: 0.6945 - mean_absolute_error: 0.6945 - val_loss: 0.7297 - val_mean_absolute_error: 0.7297\n",
      "Epoch 7/25\n",
      "4702/4702 [==============================] - 1s 148us/sample - loss: 0.7042 - mean_absolute_error: 0.7042 - val_loss: 0.7069 - val_mean_absolute_error: 0.7069\n",
      "Epoch 8/25\n",
      "4702/4702 [==============================] - 1s 147us/sample - loss: 0.6993 - mean_absolute_error: 0.6993 - val_loss: 0.7089 - val_mean_absolute_error: 0.7089\n",
      "Epoch 9/25\n",
      "4702/4702 [==============================] - 1s 157us/sample - loss: 0.6905 - mean_absolute_error: 0.6905 - val_loss: 0.6995 - val_mean_absolute_error: 0.6995\n",
      "Epoch 10/25\n",
      "4702/4702 [==============================] - 1s 177us/sample - loss: 0.6965 - mean_absolute_error: 0.6965 - val_loss: 0.7554 - val_mean_absolute_error: 0.7554\n",
      "Epoch 11/25\n",
      "4702/4702 [==============================] - 1s 171us/sample - loss: 0.7204 - mean_absolute_error: 0.7204 - val_loss: 0.7167 - val_mean_absolute_error: 0.7167\n",
      "Epoch 12/25\n",
      "4702/4702 [==============================] - ETA: 0s - loss: 0.6977 - mean_absolute_error: 0.697 - 1s 167us/sample - loss: 0.6994 - mean_absolute_error: 0.6994 - val_loss: 0.7331 - val_mean_absolute_error: 0.7331\n",
      "Epoch 13/25\n",
      "4702/4702 [==============================] - 1s 156us/sample - loss: 0.7136 - mean_absolute_error: 0.7136 - val_loss: 0.7052 - val_mean_absolute_error: 0.7052\n",
      "Epoch 14/25\n",
      "4702/4702 [==============================] - 1s 149us/sample - loss: 0.6967 - mean_absolute_error: 0.6967 - val_loss: 0.7448 - val_mean_absolute_error: 0.7448\n",
      "Epoch 15/25\n",
      "4702/4702 [==============================] - 1s 150us/sample - loss: 0.7289 - mean_absolute_error: 0.7289 - val_loss: 0.7336 - val_mean_absolute_error: 0.7336\n",
      "Epoch 16/25\n",
      "4702/4702 [==============================] - 1s 149us/sample - loss: 0.7119 - mean_absolute_error: 0.7119 - val_loss: 0.7142 - val_mean_absolute_error: 0.7142\n",
      "Epoch 17/25\n",
      "4702/4702 [==============================] - 1s 152us/sample - loss: 0.6978 - mean_absolute_error: 0.6978 - val_loss: 0.7088 - val_mean_absolute_error: 0.7088\n",
      "Epoch 18/25\n",
      "4702/4702 [==============================] - 1s 152us/sample - loss: 0.7048 - mean_absolute_error: 0.7048 - val_loss: 0.7162 - val_mean_absolute_error: 0.7162\n",
      "Epoch 19/25\n",
      "4702/4702 [==============================] - 1s 145us/sample - loss: 0.7101 - mean_absolute_error: 0.7101 - val_loss: 0.7362 - val_mean_absolute_error: 0.7362\n",
      "Epoch 20/25\n",
      "4702/4702 [==============================] - 1s 149us/sample - loss: 0.7047 - mean_absolute_error: 0.7047 - val_loss: 0.7150 - val_mean_absolute_error: 0.7150\n",
      "Epoch 21/25\n",
      "4702/4702 [==============================] - 1s 148us/sample - loss: 0.7029 - mean_absolute_error: 0.7029 - val_loss: 0.7203 - val_mean_absolute_error: 0.7203\n",
      "Epoch 22/25\n",
      "4702/4702 [==============================] - 1s 148us/sample - loss: 0.7089 - mean_absolute_error: 0.7089 - val_loss: 0.7619 - val_mean_absolute_error: 0.7619\n",
      "Epoch 00022: early stopping\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 720)               1049760   \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 360)               259560    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 180)               64980     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 905       \n",
      "=================================================================\n",
      "Total params: 1,375,205\n",
      "Trainable params: 1,375,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4702 samples, validate on 1175 samples\n",
      "Epoch 1/25\n",
      "4702/4702 [==============================] - 1s 277us/sample - loss: 0.7180 - mean_absolute_error: 0.7180 - val_loss: 0.6996 - val_mean_absolute_error: 0.6996\n",
      "Epoch 2/25\n",
      "4702/4702 [==============================] - 1s 156us/sample - loss: 0.7103 - mean_absolute_error: 0.7103 - val_loss: 0.7052 - val_mean_absolute_error: 0.7052\n",
      "Epoch 3/25\n",
      "4702/4702 [==============================] - 1s 160us/sample - loss: 0.7109 - mean_absolute_error: 0.7109 - val_loss: 0.7821 - val_mean_absolute_error: 0.7821\n",
      "Epoch 4/25\n",
      "4702/4702 [==============================] - 1s 158us/sample - loss: 0.7214 - mean_absolute_error: 0.7214 - val_loss: 0.7022 - val_mean_absolute_error: 0.7022\n",
      "Epoch 5/25\n",
      "4702/4702 [==============================] - 1s 157us/sample - loss: 0.7053 - mean_absolute_error: 0.7053 - val_loss: 0.7122 - val_mean_absolute_error: 0.7122\n",
      "Epoch 6/25\n",
      "4702/4702 [==============================] - 1s 158us/sample - loss: 0.7182 - mean_absolute_error: 0.7182 - val_loss: 0.7058 - val_mean_absolute_error: 0.7058\n",
      "Epoch 7/25\n",
      "4702/4702 [==============================] - 1s 178us/sample - loss: 0.7223 - mean_absolute_error: 0.7223 - val_loss: 0.7856 - val_mean_absolute_error: 0.7856\n",
      "Epoch 8/25\n",
      "4702/4702 [==============================] - 1s 184us/sample - loss: 0.7892 - mean_absolute_error: 0.7892 - val_loss: 0.7536 - val_mean_absolute_error: 0.7536\n",
      "Epoch 9/25\n",
      "4702/4702 [==============================] - 1s 180us/sample - loss: 0.7649 - mean_absolute_error: 0.7649 - val_loss: 0.7490 - val_mean_absolute_error: 0.7490\n",
      "Epoch 10/25\n",
      "4702/4702 [==============================] - 1s 160us/sample - loss: 0.7678 - mean_absolute_error: 0.7678 - val_loss: 0.7602 - val_mean_absolute_error: 0.7602\n",
      "Epoch 11/25\n",
      "4702/4702 [==============================] - 1s 160us/sample - loss: 0.7686 - mean_absolute_error: 0.7686 - val_loss: 0.7572 - val_mean_absolute_error: 0.7572\n",
      "Epoch 12/25\n",
      "4702/4702 [==============================] - 1s 161us/sample - loss: 0.7721 - mean_absolute_error: 0.7721 - val_loss: 0.7635 - val_mean_absolute_error: 0.7635\n",
      "Epoch 13/25\n",
      "4702/4702 [==============================] - 1s 158us/sample - loss: 0.7760 - mean_absolute_error: 0.7760 - val_loss: 0.7632 - val_mean_absolute_error: 0.7632\n",
      "Epoch 14/25\n",
      "4702/4702 [==============================] - 1s 153us/sample - loss: 0.7744 - mean_absolute_error: 0.7744 - val_loss: 0.7626 - val_mean_absolute_error: 0.7626\n",
      "Epoch 15/25\n",
      "4702/4702 [==============================] - 1s 155us/sample - loss: 0.7742 - mean_absolute_error: 0.7742 - val_loss: 0.7636 - val_mean_absolute_error: 0.7636\n",
      "Epoch 16/25\n",
      "4702/4702 [==============================] - 1s 159us/sample - loss: 0.7736 - mean_absolute_error: 0.7736 - val_loss: 0.7628 - val_mean_absolute_error: 0.7628\n",
      "Epoch 17/25\n",
      "4702/4702 [==============================] - 1s 154us/sample - loss: 0.7740 - mean_absolute_error: 0.7740 - val_loss: 0.7630 - val_mean_absolute_error: 0.7630\n",
      "Epoch 18/25\n",
      "4702/4702 [==============================] - 1s 155us/sample - loss: 0.7742 - mean_absolute_error: 0.7742 - val_loss: 0.7628 - val_mean_absolute_error: 0.7628\n",
      "Epoch 19/25\n",
      "4702/4702 [==============================] - 1s 157us/sample - loss: 0.7744 - mean_absolute_error: 0.7744 - val_loss: 0.7632 - val_mean_absolute_error: 0.7632\n",
      "Epoch 20/25\n",
      "4702/4702 [==============================] - 1s 164us/sample - loss: 0.7738 - mean_absolute_error: 0.7738 - val_loss: 0.7628 - val_mean_absolute_error: 0.7628\n",
      "Epoch 21/25\n",
      "4702/4702 [==============================] - 1s 161us/sample - loss: 0.7735 - mean_absolute_error: 0.7735 - val_loss: 0.7627 - val_mean_absolute_error: 0.7627\n",
      "Epoch 00021: early stopping\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_40 (Dense)             (None, 720)               1049760   \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 360)               259560    \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 180)               64980     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 905       \n",
      "=================================================================\n",
      "Total params: 1,375,205\n",
      "Trainable params: 1,375,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4702 samples, validate on 1175 samples\n",
      "Epoch 1/25\n",
      "4702/4702 [==============================] - 2s 337us/sample - loss: 0.7713 - mean_absolute_error: 0.7713 - val_loss: 0.7702 - val_mean_absolute_error: 0.7702\n",
      "Epoch 2/25\n",
      "4702/4702 [==============================] - 1s 188us/sample - loss: 0.7694 - mean_absolute_error: 0.7694 - val_loss: 0.7715 - val_mean_absolute_error: 0.7715\n",
      "Epoch 3/25\n",
      "4702/4702 [==============================] - 1s 176us/sample - loss: 0.7710 - mean_absolute_error: 0.7710 - val_loss: 0.7742 - val_mean_absolute_error: 0.7742\n",
      "Epoch 4/25\n",
      "4702/4702 [==============================] - 1s 194us/sample - loss: 0.7727 - mean_absolute_error: 0.7727 - val_loss: 0.7764 - val_mean_absolute_error: 0.7764\n",
      "Epoch 5/25\n",
      "4702/4702 [==============================] - 1s 183us/sample - loss: 0.7735 - mean_absolute_error: 0.7735 - val_loss: 0.7755 - val_mean_absolute_error: 0.7755\n",
      "Epoch 6/25\n",
      "4702/4702 [==============================] - 1s 166us/sample - loss: 0.7727 - mean_absolute_error: 0.7727 - val_loss: 0.7718 - val_mean_absolute_error: 0.7718\n",
      "Epoch 7/25\n",
      "4702/4702 [==============================] - 1s 155us/sample - loss: 0.7723 - mean_absolute_error: 0.7723 - val_loss: 0.7741 - val_mean_absolute_error: 0.7741\n",
      "Epoch 8/25\n",
      "4702/4702 [==============================] - 1s 156us/sample - loss: 0.7720 - mean_absolute_error: 0.7720 - val_loss: 0.7741 - val_mean_absolute_error: 0.7741\n",
      "Epoch 9/25\n",
      "4702/4702 [==============================] - 1s 165us/sample - loss: 0.7721 - mean_absolute_error: 0.7721 - val_loss: 0.7740 - val_mean_absolute_error: 0.7740\n",
      "Epoch 10/25\n",
      "4702/4702 [==============================] - 1s 183us/sample - loss: 0.7718 - mean_absolute_error: 0.7718 - val_loss: 0.7737 - val_mean_absolute_error: 0.7737\n",
      "Epoch 11/25\n",
      "4702/4702 [==============================] - 1s 176us/sample - loss: 0.7720 - mean_absolute_error: 0.7720 - val_loss: 0.7740 - val_mean_absolute_error: 0.7740\n",
      "Epoch 12/25\n",
      "4702/4702 [==============================] - 1s 162us/sample - loss: 0.7722 - mean_absolute_error: 0.7722 - val_loss: 0.7742 - val_mean_absolute_error: 0.7742\n",
      "Epoch 13/25\n",
      "4702/4702 [==============================] - 1s 156us/sample - loss: 0.7717 - mean_absolute_error: 0.7717 - val_loss: 0.7739 - val_mean_absolute_error: 0.7739\n",
      "Epoch 14/25\n",
      "4702/4702 [==============================] - 1s 159us/sample - loss: 0.7715 - mean_absolute_error: 0.7715 - val_loss: 0.7743 - val_mean_absolute_error: 0.7743\n",
      "Epoch 15/25\n",
      "4702/4702 [==============================] - 1s 159us/sample - loss: 0.7715 - mean_absolute_error: 0.7715 - val_loss: 0.7746 - val_mean_absolute_error: 0.7746\n",
      "Epoch 16/25\n",
      "4702/4702 [==============================] - 1s 157us/sample - loss: 0.7717 - mean_absolute_error: 0.7717 - val_loss: 0.7738 - val_mean_absolute_error: 0.7738\n",
      "Epoch 17/25\n",
      "4702/4702 [==============================] - 1s 161us/sample - loss: 0.7716 - mean_absolute_error: 0.7716 - val_loss: 0.7742 - val_mean_absolute_error: 0.7742\n",
      "Epoch 18/25\n",
      "4702/4702 [==============================] - 1s 160us/sample - loss: 0.7714 - mean_absolute_error: 0.7714 - val_loss: 0.7742 - val_mean_absolute_error: 0.7742\n",
      "Epoch 19/25\n",
      "4702/4702 [==============================] - 1s 159us/sample - loss: 0.7714 - mean_absolute_error: 0.7714 - val_loss: 0.7741 - val_mean_absolute_error: 0.7741\n",
      "Epoch 20/25\n",
      "4702/4702 [==============================] - 1s 151us/sample - loss: 0.7715 - mean_absolute_error: 0.7715 - val_loss: 0.7740 - val_mean_absolute_error: 0.7740\n",
      "Epoch 21/25\n",
      "4702/4702 [==============================] - 1s 153us/sample - loss: 0.7717 - mean_absolute_error: 0.7717 - val_loss: 0.7738 - val_mean_absolute_error: 0.7738\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "# K-fold\n",
    "kfold = KFold(n_splits=5).split(X_std, y_std)\n",
    "model = build_model()\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                           min_delta=0, \n",
    "                                           patience=20, \n",
    "                                           verbose=1, \n",
    "                                           mode='auto')\n",
    "\n",
    "for train_index, test_index in kfold:\n",
    "    X_train, X_val = X_std[train_index], X_std[test_index]\n",
    "    y_train, y_val = y_std[train_index], y_std[test_index]\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        batch_size=100,\n",
    "                        epochs=25,\n",
    "                        callbacks=[early_stop],\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12352451, 0.10939322, 0.10925011, 0.13513068, 0.12834675])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 評価\n",
    "preds = model.predict(X_tsf)\n",
    "preds = sc_y.inverse_transform(preds)\n",
    "scores = np.sum(np.abs(y - preds), axis=0) / np.sum(preds, axis=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル保存"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
