{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint　深層学習スクラッチ　リカレントニューラルネットワーク\n",
    "\n",
    "### ＜目的＞\n",
    "スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n",
    "\n",
    "リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n",
    "\n",
    "クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
    "h_t = tanh(a_t)\n",
    "$$\n",
    "\n",
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$x_{t}$ : 時刻tの入力 (batch_size, n_features)\n",
    "\n",
    "\n",
    "$W_{x}$ : 入力に対する重み (n_features, n_nodes)\n",
    "\n",
    "\n",
    "$h_{t-1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n",
    "\n",
    "\n",
    "$W_{h}$ : 状態に対する重み。 (n_nodes, n_nodes)\n",
    "\n",
    "\n",
    "$B$ : バイアス項 (n_nodes,)\n",
    "\n",
    "\n",
    "初期状態 $h_{0}$ は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 $x$ は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from sklearn import metrics \n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例の配列\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) \n",
    "# (batch_size, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward関数\n",
    "def forward(X, h):\n",
    "    A = X @ w_x + h @ w_h + b\n",
    "    return np.tanh(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h : [[0.79494228 0.81839002 0.83939649 0.85584174]]\n"
     ]
    }
   ],
   "source": [
    "# シーケンス数を取得し、xを変形\n",
    "n_sequence = x.shape[1]\n",
    "x = x.transpose(1,0,2)\n",
    "\n",
    "# シーケンス数だけループを回す\n",
    "# 時刻（シーケンス）t のXでhを更新していく\n",
    "for i in range(n_sequence):\n",
    "    h = forward(x[i], h)\n",
    "    \n",
    "print('h :',h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正しい出力になっているようだ。\n",
    "\n",
    "### 【問題3】（アドバンス課題）バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
    "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : $W_x$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ : $W_h$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B}$ : $B$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "\n",
    "$\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}$\n",
    "\n",
    "\n",
    "＊$\\frac{\\partial L}{\\partial h_t}$ は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    "\n",
    "\n",
    "前の時刻や層に流す誤差の数式は以下です。\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}$\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    RNNの全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "      入力の特徴量数\n",
    "    n_nodes : int\n",
    "      層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes, batch_size, initializer, activator, optimizer):\n",
    "        \n",
    "        # 各パラメーターの初期化\n",
    "        self.batch_size = batch_size\n",
    "        self.n_nodes = n_nodes\n",
    "        self.Wx = initializer.param(n_features, n_nodes)\n",
    "        self.Wh = initializer.param(n_nodes, n_nodes)\n",
    "        self.B = initializer.param(n_nodes, 1).flatten()\n",
    "        # backward時のパラメーター初期化\n",
    "        self.X = []\n",
    "        self.h = []\n",
    "        self.dWx = 0\n",
    "        self.dWh = 0\n",
    "        self.dB = 0\n",
    "        # 活性化関数、最適化手法\n",
    "        self.activator = activator\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    # 順伝播\n",
    "    def forward(self, X, h):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_features)\n",
    "            入力\n",
    "        h : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前の時刻の状態・入力\n",
    "        Returns\n",
    "        ----------\n",
    "        h : 次の形のndarray, shape (batch_size, n_features)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        # 時刻 t における変数をリストで保持\n",
    "        self.X.append(X)\n",
    "        self.h.append(h)\n",
    "        \n",
    "        # Affineと同様の計算\n",
    "        A = X @ self.Wx + h @ self.Wh + self.B\n",
    "        \n",
    "        return self.activator.activate(A)\n",
    "    \n",
    "    # 逆伝播\n",
    "    def backward(self, dX, dh, t):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dX : 次の形のndarray, shape (batch_size, n_features)\n",
    "            後ろから流れてきた勾配\n",
    "        dh : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        t  : int\n",
    "            時刻\n",
    "        Returns\n",
    "        ----------\n",
    "        dX : 次の形のndarray, shape (batch_size, n_features)\n",
    "            前に流す勾配\n",
    "        dh : 次の形のndarray, shape (batch_size, n_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 活性化関数のヤコビアンを通し、dh/da(dA)を得る\n",
    "        dA = self.activator.jacobian(dh)\n",
    "        \n",
    "        # 時刻 t における変数を取り出す\n",
    "        X = self.X[t]\n",
    "        h = self.h[t]\n",
    "        \n",
    "        # X,hとバッチサイズでE(dWx), E(dWh)を算出\n",
    "        self.dWx += (X.T @ dA) / self.batch_size\n",
    "        self.dWh += (h.T @ dA) / self.batch_size\n",
    "        self.dB += np.sum(dA, axis=0)\n",
    "        \n",
    "        # 前層に流す勾配を計算\n",
    "        dX = dA @ (self.Wx).T\n",
    "        dh = dA @ (self.Wh).T\n",
    "        \n",
    "        # t=0 になったらパラメーター更新\n",
    "        if t==0:\n",
    "            self.optimizer.update(self)\n",
    "        \n",
    "        return dX, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReccurentNeuralNetクラス\n",
    "class ScratchSipmleRNNClassifier():\n",
    "    \n",
    "    def __init__(self, n_nodes, sigma, lr):\n",
    "        self.n_nodes = n_nodes\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        \n",
    "    def fit(self, X, h, y):\n",
    "        # 入力データからシーケンス数等を取得し変形\n",
    "        batch_size, n_sequences, n_features = X.shape\n",
    "        X = X.transpose(1,0,2)\n",
    "        \n",
    "        # RNN層のインスタンス作成（単層）\n",
    "        self.rnn = SimpleRNN(n_features=n_features, \n",
    "                             n_nodes=self.n_nodes, \n",
    "                             batch_size=batch_size,\n",
    "                             initializer=SimpleInitializer(self.sigma), \n",
    "                             activator=Tanh(), \n",
    "                             optimizer=SGD(self.lr))\n",
    "        \n",
    "        # シーケンス数だけforwardし、dhのみ更新\n",
    "        for t in range(n_sequence):\n",
    "            h = self.rnn.forward(X[t], h)\n",
    "            print('\\nforward(t = {})'.format(t))\n",
    "            print('h = ', h)\n",
    "        \n",
    "        # ここはタスクによって変わるが、ひとまずは単純な差とした\n",
    "        dh = h - y\n",
    "        \n",
    "        # backward前に、dXを空の配列で初期化\n",
    "        dX = np.empty([batch_size, n_features])\n",
    "        \n",
    "        # シーケンス数だけbackwaordし、dX, dhを更新\n",
    "        # 時刻を引数に与え、t=0の時にパラメーター更新\n",
    "        for t in reversed(range(n_sequence)):\n",
    "            dX, dh = self.rnn.backward(dX, dh, t)\n",
    "            print('\\nbackward(t = {})'.format(t))\n",
    "            print('dh = ', dh)\n",
    "            print('dX = ', dX)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    \n",
    "    def predict(self, X, h):\n",
    "        # 入力データからシーケンス数等を取得し変形\n",
    "        batch_size, n_sequences, n_features = X.shape\n",
    "        X = X.transpose(1,0,2)\n",
    "        # シーケンス数だけforward\n",
    "        for i in range(n_sequence):\n",
    "            h = self.rnn.forward(X[i], h)\n",
    "        # ここでは出力をそのまま出すものとした\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパボリックタンジェント関数\n",
    "class Tanh:\n",
    "    # 順伝播時の関数メソッド（softmaxに合わせ、yも引数にとるが使用しない）\n",
    "    def activate(self, A):\n",
    "        # 逆伝播時に使用する変数を保持\n",
    "        self.Z = np.tanh(A)\n",
    "        return self.Z\n",
    "    \n",
    "    # 逆伝播時のヤコビアン計算メソッド\n",
    "    def jacobian(self, dZ):\n",
    "        dA = dZ*(1. - self.Z**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def param(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1, n_nodes_2 : int\n",
    "          パラメーターサイズ\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        param : 次の形のndarray, shape (n_nodes1, n_nodes2)\n",
    "          初期化されたパラメーター\n",
    "        \"\"\"\n",
    "        param = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある結合層のパラメーターの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : object\n",
    "          更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.Wx -= self.lr*layer.dWx\n",
    "        layer.Wh -= self.lr*layer.dWh\n",
    "        layer.B -= self.lr*layer.dB\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "forward(t = 0)\n",
      "h =  [[ 0.15319695  0.05211778 -0.05504374  0.01341288]]\n",
      "\n",
      "forward(t = 1)\n",
      "h =  [[ 0.14033102  0.04131364 -0.06229699  0.01548842]]\n",
      "\n",
      "forward(t = 2)\n",
      "h =  [[ 0.1437431   0.04180554 -0.05926674  0.01241166]]\n",
      "\n",
      "backward(t = 2)\n",
      "dh =  [[ 0.06236581  0.18715963 -0.06993082 -0.10554729]]\n",
      "dX =  [[-0.05391941  0.03441599]]\n",
      "\n",
      "backward(t = 1)\n",
      "dh =  [[-0.00909855 -0.02229231  0.00891267  0.03386782]]\n",
      "dX =  [[ 0.03840553 -0.03300507]]\n",
      "\n",
      "backward(t = 0)\n",
      "dh =  [[ 0.00263497  0.0019623   0.00051703 -0.00470682]]\n",
      "dX =  [[-0.00732095  0.00226414]]\n",
      "\n",
      "\n",
      "predict =  [[ 0.1500549   0.04876637 -0.0509964   0.02242665]]\n"
     ]
    }
   ],
   "source": [
    "# サンプル配列で確認\n",
    "X = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "h = np.zeros((1, 4)) \n",
    "y = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])\n",
    "\n",
    "# 学習\n",
    "srnn = ScratchSipmleRNNClassifier(n_nodes=4, \n",
    "                                  sigma=0.1, \n",
    "                                  lr=0.01)\n",
    "srnn.fit(X, h, y)\n",
    "\n",
    "# 推論\n",
    "h = np.zeros((1, 4)) \n",
    "pred = srnn.predict(X, h)\n",
    "print('\\n\\npredict = ', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少なくとも配列の形状は合っているようだ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
